"""
Pipeline to evaluate predictions generated by the competition_pipeline.py.
Loads predictions, benchmarks, and actuals and calculates metrics across all prediction windows.
"""

from collections.abc import Callable, Iterable
from functools import partial
import os
from typing import Literal, Any
import warnings

from joblib import Parallel, delayed
import numpy as np
import pandas as pd
import xarray as xr

from src.utils.conversion import get_date
from src.utils.data_prep import read_prio_actuals, read_benchmarks, read_predictions
from src.utils.evaluation import create_metrics_df


def bootstrap_metrics(
    observed: xr.DataArray,
    predictions_dict: dict[str, xr.DataArray],
    sample_strategy: Literal["random", "spatial"] = "random",
    sample_size: int = 1000,
    bootstrap_samples: int = 1000,
    parallel_kwargs: dict[str, Any] = {"n_jobs": 8, "verbose": 1},
) -> pd.DataFrame:
    """
    Function to bootstraps metrics based on random or spatial sampling of priogrid cells.
    Creates sample, selects data and calculates metrics. Bootstrapping implements
    parallelization via joblib.Parallel.

    Args:
        observed (xr.DataArray): observations.
        predictions_dict (dict[str, xr.DataArray]): dictionary of predictions
            for which to bootstrap metrics.
        sample_strategy (str, optional): whether to draw fully random samples ("random") or sample
            from pgids and include full time series ("spatial").
        sample_size (int, optional): number of samples (sample_strategy="random")/
            grid cells(sample_strategy="spatial") in a sample.
        bootstrap_samples (int, optional): number of boostrap samples to draw and evaluate.
        parallel_kwargs (dict[str, Any], optional): kwargs to pass to joblib.Parallel.

    Returns:
        Dataframe with metrics as index and predictions_dict keys as columns with
        each dataframe entry containing the values for all bootstrap samples for the
        given combination.
    """

    def bootstrap_parallel_spatial(sample_pgids: Iterable[int | float]) -> pd.DataFrame:
        """helper function which runs in parallel and returns the metrics for all
        included predictions in a sample.
        """
        observed_sample = observed.loc[:, sample_pgids]  # pay attention to the 1 fewer dimensions!
        predictions_dict_sample = {
            model: preds.loc[:, sample_pgids, :] for model, preds in predictions_dict.items()
        }
        df_metrics_sample = create_metrics_df(observed_sample, predictions_dict_sample)
        return df_metrics_sample

    def bootstrap_parallel_random(sample_idx: Iterable[int]) -> pd.DataFrame:
        """helper function which runs in parallel and returns the metrics for all
        included predictions in a sample.
        """
        observed_sample = observed.stack(idx=["month_id", "priogrid_gid"]).isel(idx=sample_idx)
        predictions_dict_sample = {
            model: preds.stack(idx=["month_id", "priogrid_gid"]).isel(idx=sample_idx)
            for model, preds in predictions_dict.items()
        }
        df_metrics_sample = create_metrics_df(observed_sample, predictions_dict_sample)
        return df_metrics_sample

    rng = np.random.default_rng(1465322168798791321354823142454321779949112123547241)
    pgids = next(iter(predictions_dict.values())).priogrid_gid.values
    if sample_strategy == "spatial":
        metrics_list = Parallel(**parallel_kwargs)(
            delayed(bootstrap_parallel_spatial)(rng.choice(pgids, size=sample_size, replace=True))
            for i in range(bootstrap_samples)
        )
    elif sample_strategy == "random":
        metrics_list = Parallel(**parallel_kwargs)(
            delayed(bootstrap_parallel_random)(rng.integers(0, observed.size, size=sample_size))
            for i in range(bootstrap_samples)
        )
    else:
        raise NotImplementedError(
            f"'sample_strategy' can be either 'random' or 'spatial', got {sample_strategy}."
        )

    # create unified dataframe
    metrics = metrics_list[0].index
    models = metrics_list[0].columns
    df_bootstrap_scores = pd.DataFrame(index=metrics, columns=models)
    for metric in metrics:
        for model in models:
            df_bootstrap_scores.at[metric, model] = np.array(
                [df.at[metric, model] for df in metrics_list]
            )

    return df_bootstrap_scores


def evaluation_pipeline(
    year: int,
    load_data_func: Callable[[int], tuple[xr.DataArray, dict[str, xr.DataArray]]],
    fp_out: str | None = None,
    save: bool = True,
    bootstrap: bool = True,
    bootstrap_kwargs: dict[str, Any] | None = None,
    return_results: bool = False,
    regenerate_metrics: bool = True,
) -> pd.DataFrame | tuple[pd.DataFrame, pd.DataFrame] | None:
    """
    Pipeline for evaluation against the various prediction competition and some additional metrics.
    Generates dataframes with the results and can store or return them. Optionally also
    bootstraps metrics.

    Args:
        year (int): yearly prediction window to run the pipeline for.
        load_data_func (Callable): function to load the desired data for the window. Should correspond
            to `func(year:int) -> tuple[observed: xr.DataArray, predictions: dict[str, xr.DataArray]]`.
        fp_out (str | None, optional): filepath to store evaluation results if save=True.
        save (bool, optional): whether to save the resulting dataframes.
        bootstrap (bool, optional): whether to perform bootstrapping to calculate confidence
            intervals for the metrics.
        bootstrap_kwargs (dict[str, Any], optional): dict with kwargs to be passed to
            bootstrap_metrics if bootstrap=True.
        return_results (bool, optional): whether to return the results generated.
        regenerate_metrics (bool, optional): whether to regenerate metrics if saved results already
            exist or load these results instead.

    Returns:
        pd.DataFrame | tuple[pd.DataFrame] | None: evaluation results and bootstrapped evaluation
            results, depending on `return_results` and `bootstrap` flags.
        .
    """
    if fp_out is None:
        fp_out = os.getcwd()
        if save:
            warnings.warn(
                f'No output filepath ("fp_out") defined, will save results to current directory: {os.getcwd()}'
            )
    if bootstrap_kwargs is None:
        bootstrap_kwargs = {}

    print(f"##### window {year} #####")
    fp_out_metrics = os.path.join(fp_out, "metrics")
    fp_metrics_window = os.path.join(fp_out_metrics, f"metrics_benchmarks_full_{year}.csv")
    if os.path.exists(fp_metrics_window) and not regenerate_metrics:
        print(f"Read existing prediction and benchmark metrics...")
        df_metrics = pd.read_csv(fp_metrics_window, index_col=0)
        data_loaded = False
    else:
        print("Loading predictions and creating benchmarks...")
        observed, predictions_dict = load_data_func(year)
        data_loaded = True
        print(f"Calculating prediction and benchmark metrics...")
        df_metrics = create_metrics_df(observed, predictions_dict)
        if save:
            if not os.path.exists(fp_out_metrics):
                os.makedirs(fp_out_metrics)
            df_metrics.to_csv(fp_metrics_window)

    if bootstrap:
        print("Bootstrapping metrics...")
        fp_out_bootstrap = os.path.join(fp_out, "bootstrap")
        version = bootstrap_kwargs.get("sample_strategy", "random")
        fp_bootstrap_window = os.path.join(
            fp_out, "bootstrap", f"metrics_bootstrap_{version}_{year}.parquet"
        )
        if os.path.exists(fp_bootstrap_window) and not regenerate_metrics:
            print("reading existing bootstrap scores...")
            df_bootstrap_scores = pd.read_parquet(fp_bootstrap_window)
        else:
            if not data_loaded:
                observed, predictions_dict = load_data_func(year)
            df_bootstrap_scores = bootstrap_metrics(observed, predictions_dict, **bootstrap_kwargs)  # type: ignore
            if save:
                if not os.path.exists(fp_out_bootstrap):
                    os.makedirs(fp_out_bootstrap)
                df_bootstrap_scores.to_parquet(fp_bootstrap_window)
        if return_results:
            return df_metrics, df_bootstrap_scores
    if return_results:
        return df_metrics
    else:
        return


if __name__ == "__main__":
    fp_views = "data/views_data/"
    fp_out = f"evaluation/"
    # we can of course only evaluate up to 2023
    test_windows = [2018, 2019, 2020, 2021, 2022, 2023]
    bootstrap_kwargs = {
        "sample_strategy": "random",
        "sample_size": 10000,
        "parallel_kwargs": {"n_jobs": 16, "verbose": 1},
    }

    # to be flexible in terms of which benchmarks to include etc. we define the function loading the data here
    def load_evaluation_data(year: int, fp_views: str):
        print("load predictions...")
        observed = read_prio_actuals(fp_views, year=year)

        models = ["global", "local", "global-local"]
        predictions_dict = {}
        for model in models:
            predictions_dict[model] = read_predictions(f"submissions/unibw_trees_{model}/", year)
        print(
            get_date(predictions_dict["global"].month_id.values.min()).date(),
            "to",
            get_date(predictions_dict["global"].month_id.values.max()).date(),
        )
        print("load benchmarks...")
        benchmarks = ["conflictology_n", "last", "zero", "conflictology", "boot_240"]
        views_benchmarks = {bench: read_benchmarks(fp_views, year, bench) for bench in benchmarks}

        predictions_dict = {**predictions_dict, **views_benchmarks}
        print("data loaded!")
        return observed, predictions_dict

    load_data_func = partial(load_evaluation_data, fp_views=fp_views)

    for year in test_windows:
        print("mean fatalities:", float(read_prio_actuals(fp_views, year).mean()))
        evaluation_pipeline(
            year,
            load_data_func,
            fp_out,
            save=True,
            bootstrap=False,
            regenerate_metrics=True,
            return_results=False,
            bootstrap_kwargs=bootstrap_kwargs,
        )
